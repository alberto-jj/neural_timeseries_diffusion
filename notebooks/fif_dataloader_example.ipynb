{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing FIFDataLoader for EEG Data Augmentation with DDPMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use the `FIFDataLoader` for loading EEG data from `.fif` files. We will explore setting up the dataloader for both unconditional and conditional (class-based) training of Denoising Diffusion Probabilistic Models (DDPMs).\n",
    "\n",
    "The primary goal is to simulate new EEG signals, which can be particularly useful for data augmentation, such as balancing datasets with underrepresented classes. For this demonstration, we will consider three classes: Dementia, Mild Cognitive Decline (MCI), and Healthy Controls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'ntd' is in PYTHONPATH or the notebook is run from the repo root\n",
    "from ntd.datasets import FIFDataLoader\n",
    "# from ntd.diffusion_model import DiffusionModel # Placeholder for model, if full training is shown\n",
    "# from ntd.networks import AdaConv # Placeholder for network, if full training is shown\n",
    "# import hydra # For config loading (optional, can also manually define configs)\n",
    "# from omegaconf import OmegaConf # For config loading\n",
    "\n",
    "# Configure matplotlib for inline plotting\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation: Creating Dummy .fif Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demonstration, we will create a set of dummy `.fif` files. In a real-world scenario, you would point the `FIFDataLoader` to your existing directory of `.fif` files.\n",
    "\n",
    "The dummy data will have the following parameters:\n",
    "- Sampling frequency (`sfreq`): 200 Hz\n",
    "- Number of channels (`n_channels`): 19 (standard EEG channels)\n",
    "- Epoch duration: 5 seconds (resulting in `n_times = sfreq * epoch_duration = 1000` time points per epoch)\n",
    "- Number of epochs per subject file (`n_epochs_per_subject`): 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_fif_files(base_path, class_name, subject_ids, n_epochs_per_subject=20, sfreq=200, n_channels=19, epoch_duration=5):\n",
    "    \"\"\"\n",
    "    Creates dummy .fif files for a given class and subject IDs directly in base_path.\n",
    "    Each file will contain random EEG-like data.\n",
    "    \"\"\"\n",
    "    # Files go into base_path, class information is in the filename\n",
    "    os.makedirs(base_path, exist_ok=True) # Ensure base_path exists\n",
    "    n_times = int(sfreq * epoch_duration)\n",
    "    \n",
    "    created_files = []\n",
    "    for subj_id in subject_ids:\n",
    "        # Create random data: (n_epochs, n_channels, n_times)\n",
    "        # Simulating EEG voltage levels (e.g., microvolts)\n",
    "        data = np.random.randn(n_epochs_per_subject, n_channels, n_times) * 10e-6 \n",
    "        \n",
    "        # Create MNE info object\n",
    "        ch_names = [f'EEG {i+1:02}' for i in range(n_channels)]\n",
    "        ch_types = ['eeg'] * n_channels\n",
    "        info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=ch_types)\n",
    "        info.set_montage('standard_1020', on_missing='ignore') # Add a standard montage\n",
    "        \n",
    "        # Create MNE EpochsArray\n",
    "        # Events are simplified for this dummy data\n",
    "        events = np.array([[i, 0, 1] for i in range(n_epochs_per_subject)])\n",
    "        event_id = {'dummy_event': 1}\n",
    "        epochs = mne.EpochsArray(data, info, events=events, event_id=event_id, tmin=0, verbose=False)\n",
    "        \n",
    "        # Filename convention: CLASSNAME_SUBJECTID_eeg.fif (e.g., DEMENTIA_NBS001_eeg.fif)\n",
    "        file_name = f\"{class_name.upper()}_{subj_id}_eeg.fif\" \n",
    "        file_path = os.path.join(base_path, file_name) # Save directly in base_path\n",
    "        epochs.save(file_path, overwrite=True, verbose=False)\n",
    "        created_files.append(file_path)\n",
    "        print(f\"Created: {file_path}\")\n",
    "    return base_path, created_files # Return base_path as it's the location of files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a small number of dummy subjects for each class:\n",
    "- Dementia: 2 subjects\n",
    "- Mild Cognitive Decline (MCI): 2 subjects\n",
    "- Healthy Controls (CONTROL): 2 subjects\n",
    "\n",
    "In a real scenario, you would have many more subjects per class (e.g., Dementia: 216, MCI: 318, CONTROL: 433, as per the original dataset description). The `FIFDataLoader` will use the first part of the filename (before the first `_`) as the class label if `condition_on_class_label=True`. The subject ID is typically the second part. For example, `DEMENTIA_NBS001_eeg.fif` (as created by our function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path for dummy data\n",
    "DUMMY_DATA_PARENT_DIR = \"dummy_eeg_data_notebook\" # Changed to avoid conflict with test data\n",
    "\n",
    "# Clean up old dummy data if it exists\n",
    "if os.path.exists(DUMMY_DATA_PARENT_DIR):\n",
    "    shutil.rmtree(DUMMY_DATA_PARENT_DIR)\n",
    "os.makedirs(DUMMY_DATA_PARENT_DIR)\n",
    "\n",
    "# Define subjects for each class\n",
    "class_subjects_map = {\n",
    "    \"DEMENTIA\": [\"NBS001\", \"NBS002\"], # Using \"NBS\" prefix for \"Notebook Subject\"\n",
    "    \"MCI\": [\"NBS003\", \"NBS004\"],\n",
    "    \"CONTROL\": [\"NBS005\", \"NBS006\"]\n",
    "}\n",
    "\n",
    "all_created_fif_files = []\n",
    "for class_label, subject_list in class_subjects_map.items():\n",
    "    # The function now saves files directly into DUMMY_DATA_PARENT_DIR\n",
    "    _returned_path, created_files_for_class = create_dummy_fif_files(DUMMY_DATA_PARENT_DIR, class_label, subject_list)\n",
    "    all_created_fif_files.extend(created_files_for_class)\n",
    "\n",
    "print(f\"\\nAll dummy .fif files created in parent directory: {os.path.abspath(DUMMY_DATA_PARENT_DIR)}\")\n",
    "print(f\"Total files created: {len(all_created_fif_files)}\")\n",
    "\n",
    "# For loading, we can point FIFDataLoader to DUMMY_DATA_PARENT_DIR.\n",
    "# FIFDataLoader will find all *.fif files directly in this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Data: Unconditional Case\n",
    "First, let's see how to load the data without any conditioning on subject ID or class label. We will point the dataloader to the parent directory containing all class subdirectories. `FIFDataLoader` will recursively find all `.fif` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for unconditional loading\n",
    "unconditional_config = {\n",
    "    'file_path': DUMMY_DATA_PARENT_DIR, # Point to the parent directory of all classes\n",
    "    'n_epochs': 20, # Number of epochs to load from each file\n",
    "    'condition_on_subject_id': False,\n",
    "    'condition_on_class_label': False\n",
    "}\n",
    "\n",
    "print(f\"Loading unconditional data from: {unconditional_config['file_path']}\")\n",
    "unconditional_dataset = FIFDataLoader(**unconditional_config)\n",
    "unconditional_dataloader = DataLoader(unconditional_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Total epochs loaded (unconditional): {len(unconditional_dataset)}\")\n",
    "print(f\"Number of batches: {len(unconditional_dataloader)}\")\n",
    "\n",
    "# Fetch and inspect a batch\n",
    "try:\n",
    "    batch_unconditional = next(iter(unconditional_dataloader))\n",
    "    print(f\"Batch signal shape: {batch_unconditional['signal'].shape}\") # Expected: (batch_size, n_channels, n_times)\n",
    "    print(f\"Keys in batch: {batch_unconditional.keys()}\")\n",
    "    \n",
    "    # Plot a sample from the batch\n",
    "    sample_signal_unconditional = batch_unconditional['signal'][0].numpy()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(sample_signal_unconditional[0, :]) # Plot first channel\n",
    "    plt.title(\"Sample EEG Signal (Unconditional - First Channel)\")\n",
    "    plt.xlabel(\"Time Points\")\n",
    "    plt.ylabel(\"Amplitude (Simulated)\")\n",
    "    plt.show()\n",
    "except StopIteration:\n",
    "    print(\"Dataloader is empty. Ensure dummy files were created and DUMMY_DATA_PARENT_DIR is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loading Data: Conditional on Class Label\n",
    "Now, let's configure the dataloader to be aware of class labels. This is essential for training conditional DDPMs. The `FIFDataLoader` extracts class labels from the first part of the filename (e.g., `DEMENTIA` from `DEMENTIA_NBS001_eeg.fif`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for class-conditional loading\n",
    "class_conditional_config = {\n",
    "    'file_path': DUMMY_DATA_PARENT_DIR,\n",
    "    'n_epochs': 20,\n",
    "    'condition_on_subject_id': False, # Can also be True\n",
    "    'condition_on_class_label': True\n",
    "}\n",
    "\n",
    "print(f\"Loading class-conditional data from: {class_conditional_config['file_path']}\")\n",
    "class_conditional_dataset = FIFDataLoader(**class_conditional_config)\n",
    "class_conditional_dataloader = DataLoader(class_conditional_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Total epochs loaded (class-conditional): {len(class_conditional_dataset)}\")\n",
    "print(f\"Number of batches: {len(class_conditional_dataloader)}\")\n",
    "    \n",
    "# Inspect label mapping\n",
    "if hasattr(class_conditional_dataset, 'label_to_int_id'):\n",
    "    print(f\"Class label to integer ID mapping: {class_conditional_dataset.label_to_int_id}\")\n",
    "\n",
    "# Fetch and inspect a batch\n",
    "try:\n",
    "    batch_class_conditional = next(iter(class_conditional_dataloader))\n",
    "    print(f\"Batch signal shape: {batch_class_conditional['signal'].shape}\")\n",
    "    print(f\"Keys in batch: {batch_class_conditional.keys()}\")\n",
    "    print(f\"Batch class labels: {batch_class_conditional['class_label']}\")\n",
    "    print(f\"Batch class labels shape: {batch_class_conditional['class_label'].shape}\")\n",
    "\n",
    "    # Plot a sample and show its class\n",
    "    sample_idx = 0\n",
    "    sample_signal_class = batch_class_conditional['signal'][sample_idx].numpy()\n",
    "    sample_label_int = batch_class_conditional['class_label'][sample_idx].item()\n",
    "    \n",
    "    # Reverse map integer label to string label for display\n",
    "    if hasattr(class_conditional_dataset, 'label_to_int_id'):\n",
    "        int_to_label_id = {v: k for k, v in class_conditional_dataset.label_to_int_id.items()}\n",
    "        sample_label_str = int_to_label_id.get(sample_label_int, 'Unknown')\n",
    "    else:\n",
    "        sample_label_str = 'Unknown (mapping not found)'\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(sample_signal_class[0, :]) # Plot first channel\n",
    "    plt.title(f\"Sample EEG Signal (Class: {sample_label_str} [{sample_label_int}] - First Channel)\")\n",
    "    plt.xlabel(\"Time Points\")\n",
    "    plt.ylabel(\"Amplitude (Simulated)\")\n",
    "    plt.show()\n",
    "except StopIteration:\n",
    "    print(\"Dataloader is empty. Ensure dummy files were created and DUMMY_DATA_PARENT_DIR is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loading Data: Conditional on Subject ID\n",
    "Similarly, we can condition on subject IDs. The `FIFDataLoader` extracts subject IDs from the second part of the filename (e.g., `NBS001` from `DEMENTIA_NBS001_eeg.fif`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for subject-conditional loading\n",
    "subject_conditional_config = {\n",
    "    'file_path': DUMMY_DATA_PARENT_DIR,\n",
    "    'n_epochs': 20,\n",
    "    'condition_on_subject_id': True,\n",
    "    'condition_on_class_label': False # Can also be True\n",
    "}\n",
    "\n",
    "print(f\"Loading subject-conditional data from: {subject_conditional_config['file_path']}\")\n",
    "subject_conditional_dataset = FIFDataLoader(**subject_conditional_config)\n",
    "subject_conditional_dataloader = DataLoader(subject_conditional_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "print(f\"Total epochs loaded (subject-conditional): {len(subject_conditional_dataset)}\")\n",
    "print(f\"Number of batches: {len(subject_conditional_dataloader)}\")\n",
    "    \n",
    "# Inspect subject ID mapping\n",
    "if hasattr(subject_conditional_dataset, 'subject_str_to_int_id'):\n",
    "    print(f\"Subject string to integer ID mapping: {subject_conditional_dataset.subject_str_to_int_id}\")\n",
    "\n",
    "# Fetch and inspect a batch\n",
    "try:\n",
    "    batch_subject_conditional = next(iter(subject_conditional_dataloader))\n",
    "    print(f\"Batch signal shape: {batch_subject_conditional['signal'].shape}\")\n",
    "    print(f\"Keys in batch: {batch_subject_conditional.keys()}\")\n",
    "    print(f\"Batch subject IDs: {batch_subject_conditional['subject_id']}\")\n",
    "    print(f\"Batch subject IDs shape: {batch_subject_conditional['subject_id'].shape}\")\n",
    "\n",
    "    # Plot a sample and show its subject ID\n",
    "    sample_idx = 0\n",
    "    sample_signal_subj = batch_subject_conditional['signal'][sample_idx].numpy()\n",
    "    sample_subj_id_int = batch_subject_conditional['subject_id'][sample_idx].item()\n",
    "    \n",
    "    # Reverse map integer ID to string ID for display\n",
    "    if hasattr(subject_conditional_dataset, 'subject_str_to_int_id'):\n",
    "        int_to_subj_id_str = {v: k for k, v in subject_conditional_dataset.subject_str_to_int_id.items()}\n",
    "        sample_subj_id_str = int_to_subj_id_str.get(sample_subj_id_int, 'Unknown')\n",
    "    else:\n",
    "        sample_subj_id_str = 'Unknown (mapping not found)'\n",
    "        \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(sample_signal_subj[0, :]) # Plot first channel\n",
    "    plt.title(f\"Sample EEG Signal (Subject ID: {sample_subj_id_str} [{sample_subj_id_int}] - First Channel)\")\n",
    "    plt.xlabel(\"Time Points\")\n",
    "    plt.ylabel(\"Amplitude (Simulated)\")\n",
    "    plt.show()\n",
    "except StopIteration:\n",
    "    print(\"Dataloader is empty. Ensure dummy files were created and DUMMY_DATA_PARENT_DIR is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Next Steps: Training a DDPM and Simulating New Signals\n",
    "\n",
    "With the `FIFDataLoader` set up and capable of providing EEG data in the required format (and optionally class/subject labels), the next stage involves training a Denoising Diffusion Probabilistic Model (DDPM) and then using it to simulate new EEG signals. This is particularly powerful for data augmentation, such as balancing datasets with underrepresented classes.\n",
    "\n",
    "### Conceptual DDPM Training\n",
    "\n",
    "The core idea is to train a model that learns to reverse a noise process. The `signal` tensor from our `FIFDataLoader` (e.g., `batch['signal']`) serves as the clean data input to this process.\n",
    "\n",
    "#### A. Unconditional Model Training\n",
    "\n",
    "If the goal is to generate diverse EEG signals representative of the entire dataset distribution (without targeting specific classes or subjects):\n",
    "\n",
    "1.  **Data:** Use `FIFDataLoader` with `condition_on_class_label=False` and `condition_on_subject_id=False`.\n",
    "2.  **Network Architecture:**\n",
    "    *   A good starting point could be adapting a configuration like `conf/network/ada_conv_ner.yaml`.\n",
    "    *   **Key parameters to adjust:**\n",
    "        *   `signal_channel`: Set this to **19** for our EEG data.\n",
    "        *   `in_kernel_size`, `slconv_kernel_size`, `num_scales`: These may need tuning based on your data's specific characteristics (e.g., complexity, temporal dependencies). Refer to the Q&A email for guidance on these.\n",
    "3.  **Model:** The DDPM would typically be an instance of a class like `ntd.diffusion_model.DiffusionModel`, which wraps the chosen network (e.g., `ntd.networks.AdaConv`).\n",
    "4.  **Training Loop:** This involves:\n",
    "    *   An optimizer (e.g., AdamW).\n",
    "    *   Iteratively feeding batches of signals to the `DiffusionModel`.\n",
    "    *   Calculating and minimizing the diffusion loss, which trains the network to predict and remove the added noise at each step of the diffusion process.\n",
    "\n",
    "#### B. Class-Conditional Model Training (for Data Augmentation)\n",
    "\n",
    "To generate data for *specific* classes (e.g., to augment the 'DEMENTIA' class):\n",
    "\n",
    "1.  **Data:** Use `FIFDataLoader` with `condition_on_class_label=True`. This provides both `signal` and `class_label` tensors in each batch.\n",
    "2.  **Network Architecture:**\n",
    "    *   A starting point could be `conf/network/ada_conv_tycho.yaml`.\n",
    "    *   **Key parameters to adjust:**\n",
    "        *   `signal_channel`: Set to **19**.\n",
    "        *   `cond_dim`: This is crucial. Set it to the **number of unique classes** in your dataset (e.g., 3 for 'DEMENTIA', 'MCI', 'CONTROL'). The `FIFDataLoader` makes the integer-mapped class labels available (e.g., in `class_conditional_dataset.label_to_int_id`).\n",
    "        *   Other parameters like `in_kernel_size`, `slconv_kernel_size` as needed.\n",
    "3.  **Model:** The `DiffusionModel` (wrapping a network like `AdaConv`) would be configured to accept these class labels as conditional input. The network internally uses this conditional information (often via adaptive layer normalization or similar mechanisms) to guide the denoising process.\n",
    "4.  **Training Loop:** Similar to unconditional training, but the `class_label` tensor is passed to the model along with the `signal` tensor during each training step.\n",
    "\n",
    "#### C. Subject-Conditional Model Training (Brief Mention)\n",
    "\n",
    "For generating data specific to individual subjects:\n",
    "\n",
    "1.  **Data:** Use `FIFDataLoader` with `condition_on_subject_id=True`.\n",
    "2.  **Network & Model:** Similar to class-conditional, but `cond_dim` in the network configuration would correspond to the number of unique subjects. The `subject_id` tensor would be used as the condition. This can be useful for generating more data for subjects with limited recordings.\n",
    "\n",
    "### Simulating New EEG Signals\n",
    "\n",
    "Once your DDPM is trained:\n",
    "\n",
    "1.  **Unconditional Model:**\n",
    "    *   Call the model's `sample(num_samples)` method (or a similar generation function).\n",
    "    *   This will generate `num_samples` new EEG epochs, each reflecting the general characteristics of the training data.\n",
    "2.  **Class-Conditional Model:**\n",
    "    *   Call `sample(num_samples, condition_labels)` (actual signature might vary).\n",
    "    *   `condition_labels` would be a tensor of integer class IDs for which you want to generate data. For example, to generate 100 new 'DEMENTIA' epochs, you'd pass the integer ID corresponding to 'DEMENTIA' repeated 100 times.\n",
    "    *   This is the key to targeted data augmentation for balancing classes.\n",
    "\n",
    "### Configuration Files\n",
    "\n",
    "*   Remember to set up your experiment using Hydra configuration files.\n",
    "*   Start with `conf/dataset/fif_data_example.yaml` for the dataset component, ensuring `file_path` points to your data and conditional flags are set correctly.\n",
    "*   Create or adapt network configuration files (e.g., from `conf/network/ada_conv_ner.yaml` or `ada_conv_tycho.yaml`) adjusting `signal_channel`, `cond_dim` (if conditional), and other hyperparameters like `in_kernel_size`, `slconv_kernel_size`, `num_scales` as discussed in project documentation or Q&A.\n",
    "*   Set up the main experiment configuration file that ties together the dataset, network, diffusion, and training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Conceptual Placeholder Code Snippets for Model Training ---\n",
    "# Note: These are highly simplified and illustrative. \n",
    "# Actual implementation depends on your project's training scripts and Hydra setup.\n",
    "\n",
    "# --- Unconditional Model Example ---\n",
    "print(\"\\n--- Conceptual Unconditional Model Setup ---\")\n",
    "# 1. Configuration (Illustrative - assuming Hydra is used elsewhere to load these)\n",
    "#    Typically, you'd load a main config file that composes dataset, network, diffusion configs.\n",
    "#    For example, if you have 'conf/experiments/my_unconditional_exp.yaml':\n",
    "#    # import hydra\n",
    "#    # from omegaconf import OmegaConf\n",
    "#    # @hydra.main(config_path=\"../../conf\", config_name=\"experiments/my_unconditional_exp.yaml\") # Adjust path as needed\n",
    "#    # def my_app(cfg):\n",
    "#    #    dataset_cfg = cfg.dataset \n",
    "#    #    network_cfg = cfg.network \n",
    "#    #    diffusion_cfg = cfg.diffusion\n",
    "\n",
    "# 2. Dataloader (using settings from this notebook for unconditional case)\n",
    "#    (Make sure DUMMY_DATA_PARENT_DIR is still populated if running this cell standalone)\n",
    "if os.path.exists(DUMMY_DATA_PARENT_DIR):\n",
    "    unconditional_ds_placeholder = FIFDataLoader(\n",
    "        file_path=DUMMY_DATA_PARENT_DIR, \n",
    "        n_epochs=20, \n",
    "        condition_on_class_label=False, \n",
    "        condition_on_subject_id=False\n",
    "    )\n",
    "    unconditional_loader_placeholder = DataLoader(unconditional_ds_placeholder, batch_size=4, shuffle=True)\n",
    "    print(f\"Unconditional DataLoader ready: {len(unconditional_ds_placeholder)} samples.\")\n",
    "\n",
    "    # 3. Network & Diffusion Model (Conceptual - actual classes from ntd.*)\n",
    "    #    from ntd.networks import AdaConv # Example network\n",
    "    #    from ntd.diffusion_model import DiffusionModel # Example model\n",
    "    #    # Assuming network_cfg and diffusion_cfg are OmegaConf dicts from Hydra or manually defined\n",
    "    #    # network_cfg_dict = {'signal_channel': 19, 'in_kernel_size': 17, ...} # Simplified example\n",
    "    #    # diffusion_cfg_dict = {'num_diff_steps': 1000, ...} # Simplified example\n",
    "    #    # network = AdaConv(**network_cfg_dict)\n",
    "    #    # model = DiffusionModel(network=network, **diffusion_cfg_dict) \n",
    "    #    print(f\"Conceptual Unconditional DiffusionModel instantiated (placeholder). Actual instantiation requires full configs.\")\n",
    "    #    # Training loop would follow:\n",
    "    #    # optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    #    # for epoch in range(num_epochs):\n",
    "    #    #     for batch in unconditional_loader_placeholder:\n",
    "    #    #         signals = batch['signal']\n",
    "    #    #         loss = model.compute_loss(signals) # Or model(signals) depending on API\n",
    "    #    #         optimizer.zero_grad()\n",
    "    #    #         loss.backward()\n",
    "    #    #         optimizer.step()\n",
    "else:\n",
    "    print(f\"Dummy data directory {DUMMY_DATA_PARENT_DIR} not found. Skipping unconditional conceptual setup.\")\n",
    "\n",
    "\n",
    "# --- Class-Conditional Model Example ---\n",
    "print(\"\\n--- Conceptual Class-Conditional Model Setup ---\")\n",
    "# 1. Configuration (Illustrative)\n",
    "#    # @hydra.main(config_path=\"../../conf\", config_name=\"experiments/my_conditional_exp.yaml\") \n",
    "#    # def my_conditional_app(cfg_cond):\n",
    "#    #    dataset_cfg_cond = cfg_cond.dataset\n",
    "#    #    network_cfg_cond = cfg_cond.network # Ensure cond_dim is set (e.g., to 3 for our classes)\n",
    "#    #    diffusion_cfg_cond = cfg_cond.diffusion\n",
    "\n",
    "# 2. Dataloader (using settings from this notebook for class-conditional case)\n",
    "if os.path.exists(DUMMY_DATA_PARENT_DIR):\n",
    "    class_conditional_ds_placeholder = FIFDataLoader(\n",
    "        file_path=DUMMY_DATA_PARENT_DIR, \n",
    "        n_epochs=20, \n",
    "        condition_on_class_label=True,\n",
    "        condition_on_subject_id=False\n",
    "    )\n",
    "    class_conditional_loader_placeholder = DataLoader(class_conditional_ds_placeholder, batch_size=4, shuffle=True)\n",
    "    print(f\"Class-Conditional DataLoader ready: {len(class_conditional_ds_placeholder)} samples.\")\n",
    "    if hasattr(class_conditional_ds_placeholder, 'label_to_int_id'):\n",
    "        print(f\"Class mapping: {class_conditional_ds_placeholder.label_to_int_id}\")\n",
    "    \n",
    "    # Fetch a sample batch to show structure\n",
    "    try:\n",
    "        first_batch_cond = next(iter(class_conditional_loader_placeholder))\n",
    "        signals_cond = first_batch_cond['signal']\n",
    "        labels_cond = first_batch_cond['class_label']\n",
    "        print(f\"Sample conditional batch - signals shape: {signals_cond.shape}, labels: {labels_cond}\")\n",
    "    except StopIteration:\n",
    "        print(\"Conditional loader is empty (perhaps DUMMY_DATA_PARENT_DIR is empty).\")\n",
    "\n",
    "    # 3. Network & Diffusion Model (Conceptual)\n",
    "    #    # network_cfg_cond_dict = {'signal_channel': 19, 'cond_dim': 3, ...} # Example for 3 classes\n",
    "    #    # network_cond = AdaConv(**network_cfg_cond_dict)\n",
    "    #    # model_cond = DiffusionModel(network=network_cond, **diffusion_cfg_dict) # diffusion_cfg_dict from above\n",
    "    #    print(f\"Conceptual Class-Conditional DiffusionModel instantiated (placeholder). Actual instantiation requires full configs.\")\n",
    "    #    # Training loop would pass labels:\n",
    "    #    # optimizer_cond = torch.optim.AdamW(model_cond.parameters(), lr=1e-4)\n",
    "    #    # for epoch in range(num_epochs):\n",
    "    #    #     for batch in class_conditional_loader_placeholder:\n",
    "    #    #         signals = batch['signal']\n",
    "    #    #         class_labels = batch['class_label']\n",
    "    #    #         loss = model_cond.compute_loss(signals, cond=class_labels) # Or model(signals, cond=class_labels)\n",
    "    #    #         optimizer_cond.zero_grad()\n",
    "    #    #         loss.backward()\n",
    "    #    #         optimizer_cond.step()\n",
    "    #\n",
    "    #    # Simulating data for a specific class (e.g., class_id 0 - DEMENTIA)\n",
    "    #    # num_new_samples = 10\n",
    "    #    # class_id_to_generate = 0 \n",
    "    #    # target_class_id_tensor = torch.tensor([class_id_to_generate] * num_new_samples, dtype=torch.long)\n",
    "    #    # synthetic_signals = model_cond.sample(num_samples=num_new_samples, condition_labels=target_class_id_tensor)\n",
    "    #    # print(f\"Conceptual: Generated {synthetic_signals.shape[0]} samples for class ID {class_id_to_generate}.\")\n",
    "else:\n",
    "    print(f\"Dummy data directory {DUMMY_DATA_PARENT_DIR} not found. Skipping class-conditional conceptual setup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup\n",
    "Remove the dummy data directory created for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(DUMMY_DATA_PARENT_DIR):\n",
    "    shutil.rmtree(DUMMY_DATA_PARENT_DIR)\n",
    "    print(f\"Cleaned up dummy data directory: {DUMMY_DATA_PARENT_DIR}\")\n",
    "else:\n",
    "    print(\"Dummy data directory not found, no cleanup needed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
